\chapter{Approach}\label{chap:chap4}

\section*{}
This chapter focus on the description of the approach used to solve the
problem presented for this thesis. It begins with an high level architecture
description and a brief description of the main phases that take part of the
proposed approach.

This section is followed by a more low level architecture of each one of the phases.
Of each phase, its main flow is described, its principal variables, constrains and
used methods, algorithms and their objective.

It concludes with a description of the experimental setup for the whole approach
and for each component; this last part is an introduction to the next
chapter where the results are discussed and analysed.

\section{High-level Architecture}

\begin{figure}[h] \begin{center} \leavevmode
\includegraphics[]{high_level} \caption{ High level overview
of the approach } \label{fig:highlevel_arch} \end{center} \end{figure}

As the figure~\ref{fig:highlevel_arch} makes clear, the goal of the
proposed approach is to use a past dataset containing the log of the web
activity of an online advertising related network to generate a representation of a
possible future web activity on the same network. In such a way, that the
tendencies and the data coherency is preserved.

This approach can be divided three main phases, the first, \emph{segmentation}, is
optional, its main purpose is divide the dataset in smaller and more predictable
datasets in order to improve the results of the second phase, mostly when there
are large quantities of data available.

The second phase is where the \emph{forecast of the volumes} that characterize the traffic on
the network are done, using time series prediction method.

The third an last phase of the process, the more complex one, is where the volumes
generated from the phase two combined with the data provided by the original
dataset are used to \emph{generate a dataset} that represent a possible future of the
web activity on the target network.

\section{Architecture for Web Activity Forecasting and Synthesising}

\subsection{Data Segmentation}

\begin{figure}[h] \begin{center} \leavevmode
\includegraphics[]{segmentation} \caption{ High level overview
of Data Segmentation} \label{fig:segmentation_arch} \end{center} \end{figure}

The \emph{Data Segmentation} phase was designed in order to achieve better
results in the following phases. 

In order to get better results in the second phase it is need that the time
series obtained from the dataset has certain characteristics that allow it to be
more predictable, a trend and/or a recurring pattern. One way of get the results
is by split the data by some \textbf{parameter} that is known to have effect on the
seasonality of the traffic, for example the website that is being accessed.

As the figure~\ref{fig:segmentation_arch} shows the method described before was
one of the approaches chosen to achieve a more predictable time series. This
method allows to select any parameter available in the dataset and distribute
the impressions clusters. All the members of each cluster share the same value
of the selected parameter. To achieve a good result using this method a great
knowledge about the available impressions is needed.

The other approach that was implemented on this phase is a simplified and
striped down version of a \emph{data stream clustering} algorithm. This
algorithm is distance based, and since the structure of the datasets used on
this thesis are not guaranteed the distance measure used is na\"{i}ve. The
distance between any to impressions can be described as:

\begin{center}
\begin{equation*}
  distance_{x,y}= \sum\limits_{i=0}^n d_i
\end{equation*}
\begin{equation*}
\text{where } d_i = \begin{cases} \text{1 if } x_i = y_x\\
\text{0 otherwise}\end{cases} \text{ and x, y are two impressions from the
dataset}
\end{equation*}
\end{center}

\begin{algorithm}
  \LinesNumbered
  \SetNlSty{texttt}{(}{)}
  \KwData{$lines$, all impressions available on the dataset; $threshold$,
  maximum distance to be considered member of a certain cluster.}
  \KwResult{A list of clusters, each one characterized by the $centroid$ and a list of
  offsets representing the position of each impression on the dataset.}
  \BlankLine

  \Repeat{no more impressions}{
    compare each impression with the existing list of clusters\;
    if $dist < threshold$ then add to the selected cluster, else create a new
    cluster and use the selected impression as the centroid for the new
    cluster\;
  }
  \BlankLine

  \caption[Data stream clustering]{
    Data stream clustering simplified algorithm to aggregate the impressions by
    the parameters that they have in common.
  }
  \label{alg:pam} \end{algorithm}

This kind of algorithm was selected because of the constrains imposed by the
problem, the most important ones the huge volume of data than this approach
might need to process, the number and type of attributes that compose an
impression can be different from dataset to dataset. The huge volume of data
makes impossible the usage for example of a dissimilarity matrix due to memory
constrains, the uncertainty of the parameters that are available also limit the
quality of the distance measure that assumes that every parameter has the same
weight as the others.

This family of clustering algorithms has some known limitations, for example, the
order in which the dataset is read directly influences the outcome of the
algorithm. Also since the centroids are never updated, as they are represented
by the attributes of the impression that originated that particular centroid,
when a new impression has a distance lower than the threshold when compared with
a certain centroid it is not assured that the respective cluster is the more
similar to the new impression, to address that possible issue the possibility of
search over all available centroids and select the one with a smaller distance
is also available.

\subsection{Volume Forecasting}

\begin{figure}[h] \begin{center} \leavevmode
\includegraphics[]{forecast_volumes_arch} \caption{ High level overview
of Data Segmentation} \label{fig:forecast_volumes_arch} \end{center} \end{figure}

One way of describe the impressions is by representing how many happen during a
certain amount of time. So the impressions in the past can be described as time
series, of volume of impressions in time spaced at uniform time intervals. 

So to predict how many impressions will happen in the future time series
analyses techniques can be used. 

Although, the volume of impressions doesn't allow to fully characterize the future so
 it is needed use more time series of different variables that can provide more
 information about each epoch.

The approach here described involves the usage of three time series, one for the
number of impressions, other for the percentage unique users and a last one for
the percentage of unique users that had never appeared in the past.

\paragraph{Number of impressions}
is how many entries are on the dataset during each time interval.

\paragraph{Percentage of unique users}
is described by $\frac{\text{Number of users}}{\text{Number of impressions}}
* 100$, and represents how the number of users are related with the number of
impressions.

\paragraph{Percentage of new users}
represents per unit of time how many users that had never appeared before are
represented. It is described by $\frac{\text{Number of new users}}{\text{Number
of unique users}}*100$.
\\

So each unit of time can be represented by this three values, now to
characterize the future it is needed to forecast each one of this three values
on future time units. To complete this task the \emph{ARIMA} approach was used.


******WHY DID I USED ARIMA

\subsection{Dataset Generator}

\begin{figure}[h] \begin{center} \leavevmode
\includegraphics[]{high_level_file_gen} \caption{ High level overview
of the Dataset Generator } \label{fig:highlevel_arch_file_gen} \end{center} \end{figure}

palceholder

\subsubsection{Pre-process (i)}

\begin{figure}[h] \begin{center} \leavevmode
\includegraphics[]{pre_processing_i} \caption{ High level overview
of the Dataset Pre-processing} \label{fig:pre_processing_i} \end{center} \end{figure}

palceholder

\subsubsection{Calculate Statistics (ii)}

\begin{figure}[h] \begin{center} \leavevmode
\includegraphics[]{calculate_stats} \caption{ High level overview
of statistics calculation} \label{fig:calculate_stats_ii} \end{center} \end{figure}

palceholder

\subsubsection{Fill Future Data (iii)}

\begin{figure}[h] \begin{center} \leavevmode
\includegraphics[]{fill_future} \caption{ High level overview
of fill future data} \label{fig:fill_future_iii} \end{center} \end{figure}

placeholder


\section{Component Testing}

\subsection{Data Segmentation}

\subsection{Volume Forecasting}

\subsection{Dataset Generator}


\section{Results and Conclusions}

