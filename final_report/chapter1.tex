\chapter{Introduction} \label{chap:intro}


\section{Context and Framing} \label{sec:context}

The online marketing is a growing multibillion-dollar industry \cite{PricewaterhouseCoopers2013}
which is expected to continue its fast growth\cite{PricewaterhouseCoopers2013a}.

This industry is always trying to become more efficient by getting more profit from
assets it already owns. 
Web users are the major assets of this industry, which makes money by exploiting the user behavior and characteristics, in order to target them with the
perfect campaign. Each campaign has its own target parameters, which limit the target user universe.
Online marketing industry core business is centered in web users and this industry has recorded almost every footprint each user makes on the web.
Future footprints of the web users allows the measurement of the behavior of an
upcoming campaign and, with this data, it is possible to make the inventory more
profitable. Therefore, using future user data allows the adtech industry to be able to fine tune its campaigns. 
Campaigns are composed by a series of ads that share the same main idea they
want to transmit. The campaigns have a targeting typically defined as a set of
parameter definitions and rules. To be able to run the campaigns in a
simulator, their targeting can be defined as
queries over the ad requests' data. The utilization of a simulation allows to
get the results fast, test concurrent campaigns and test multiple scenarios.
The utilization of a simulation is the main reason behind
why is so important to be able to generate future ad requests' data.

The most common platforms that will benefit from this data are Custom-Built Ad
Servers and \hyperref[itm:adex]{Exchanges}, \hyperref[itm:ssp]{Sell-Side Platforms} (SSPs) and
\hyperref[itm:dsp]{Demand-Side Platforms}
(DSPs). These platforms are further explored in section~\ref{sec:adover}.


\section{Project} \label{sec:proj}

The online advertising market is huge and its size has been increasing in money,
campaigns and users. Both platforms that sell and buy space for ad placement
want to understand what is their value and more important, what will be their
value in the future. In both cases this value is mostly constrain by campaigns
and users that are target for their campaigns.

Our goal is to forecast the availability of the users in the future so we can
simulate the value of future campaigns over them.

Since we do not know which characteristics the future campaigns will have, there
is the need to forecast every detail available on the impressions, in order to
obtain the correct values when the queries are executed over the generated data.

So, the main goal is to be able to generate a dataset able to be used on a campaign
simulator, so we must be able to predict the values with the maximum detail
possible.

This simulator needs to have available every detail possible about every
impression in order to identify which impressions are compatible with each
campaign. The result would be expressed by number of impressions per campaign and
users target by every campaign, over time.

The approach should also be independent on any parameter other than the
date and the user id of the impression.
This constraint is imposed by the multiple sources of the dataset used, since
each one of this sources could store different details and different types of
parameters about each impressions, so we cannot really on the availability of
such parameters.

The approach should be able to generate data for any source
with any parameters, based only on historical information.

In form of conclusion, the main objectives are:
\begin{itemize}
\item Correctly predict volumes of activity on an ad network for a given time in
  the future based only in past data;
\item Fill the volumes with impressions, with the maximum detail possible, to be
  able to use the obtained result on a simulator.
\end{itemize}

%The main objective of this project is to develop a library capable of generating
%future ad requests' logs from past recorded requests.

%In order to address this problem multiple ways to aggregate impressions
%will be used,
%so they become more predictable. These aggregated impressions will be transformed in time
%series, to which suitable analysis methods are applied, in order to predict
%their future values.
%In order to give more detail to the results the generator will also be able to
%mimic past characteristics of the dataset into the future in order to capture
%behaviours that were invisible to the time series.

%This generator will use real ad request' logs from various sources, stored in files.
%The generated files must respect the same structure as the original files.

%The focus of this thesis is generating future ad requests' logs, which is just
%one part of the more complex problem of
%forecasting the behaviour of campaigns in the future. The generated results can
%later be
%used on a simulator of online advertising campaigns.
%The resultant ad request log
%will be validated using a simulator supplied by ShiftForward.


%The project consists of creating a library capable of predicting the future access data logs based only on data logs of previous months.
%The hearth of the project lies in applying various data mining techniques to identify what will be the behavior of the users on the future,
%based on their behavior on the past.

%There are three main components on this project
%\begin{itemize}
%\item \textbf{The original dataset}, which is a CSV file with registry of accesses for a given network. Usually, there is 
  %a time, an user ID, a location ID, a URL and some additional parameters (cookies,etc. which may or may not be present from dataset to dataset) per entry.
%\item \textbf{A library capable of generating real like data} logs of future data based only on the past data. This is the work that will be developed during
  %this dissertation. The generation of this data logs must rely on the utilization of data mining techniques.
%\item \textbf{A simulator} capable of running campaigns over the generated dataset, to give advertising performance metrics to compare with real data for validation.
%\end{itemize}

\section{Motivation and Goals} \label{sec:goals}

In the last few years, the online marketing has been getting more complex. In
such a way that today campaigns have a very well defined target, with
sets of rules and limitations. This poses a big problem to
simpler prediction models that normally don't predict all the parameters of the
ad request, this way limiting the parameters where queries can be done.


Nowadays, some online ads can only be imprinted if a set of very specific requirements has been fulfilled, for example,
the users had to visit an e-commerce site in the last 24 hours. This brings causality into the equation, creating a new paradigm that makes 
the more traditional methods of prediction ineffective. To solve this problem
and to be able to get fast responses to complex queries of concurrent campaigns,
simulate the algorithms executed by ad servers of the client
and ultimately parallelize the computation of the results for the
queries,
the complete future data has to be predicted. This generated data
can be used in simulations and the online campaigns can run on top of the future population.

The objective of this thesis is to develop a library capable of generating
future ad request logs using past data from the same network.
This library will have as one of its main goals the prediction of all the parameters that characterize an
ad request with the purpose of being able to query over any parameter, in other
words, the generated dataset (ad requests log) must have the same attributes as the original.

The prediction of this kind of future data is rather complex since it is
necessary to find out which users will appear in the future and which websites they
will visit and when will they do it.

\section{Report Structure} \label{sec:struct}

Besides this first introductory chapter, this report is divided into four additional chapters.
In Chapter~\ref{chap:sota} it is explained some basic knowledge about online marketing. In addiction, there is
a small discussion on which of those methods are more adequate to help solve this problem.
%Chapter~\ref{chap:chap3} focus on give a better description of the problem and
%present the main objectives.
Chapter~\ref{chap:chap4} describes the proposed approach and also explains how
the experimental setup was configured and designed.
Chapter~\ref{chap:experiments} focus on the results and their analyses, it is on
this chapter where the results of all the phases of the approach are evaluated.
Chapter~\ref{chap:concl} sums up the report, giving a better context of all the
review done in the final project, it also includes some suggestions for future
work that can be done to improve the proposed solution.

